{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(descripcion):\n",
    "    \n",
    "    descripcion = descripcion.replace(',', ' ')\n",
    "    descripcion = descripcion.replace('.', ' ')\n",
    "    descripcion = descripcion.replace('<', ' ')\n",
    "    descripcion = descripcion.replace('>', ' ')\n",
    "    descripcion = descripcion.replace('\\n', ' ')\n",
    "    descripcion = descripcion.replace('\\strong', ' ')\n",
    "    descripcion = descripcion.replace('\\bold', ' ')\n",
    "    descripcion = descripcion.lower()\n",
    "    \n",
    "    return descripcion\n",
    "\n",
    "def contar_palabras(titulo, palabras):\n",
    "    \n",
    "    contadas = 0\n",
    "    \n",
    "    for palabra in palabras:\n",
    "        spliteado = titulo.split()\n",
    "        contadas += spliteado.count(palabra)\n",
    "        \n",
    "    return contadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads\n",
    "train.csv -> train\n",
    "<br>\n",
    "test.csv -> test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparo el modelo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Direción:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    237759\n",
       "0      2241\n",
       "Name: direccion, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRINVALIDAS = set(['Calle','domicilio conocido','calle','CALLE','--','Sin nombre ','1','.','-','x #x','0','...','S/N'])\n",
    "\n",
    "df = train\n",
    "\n",
    "df.loc[df.direccion.isin(DIRINVALIDAS), 'direccion'] = 0\n",
    "df.loc[df['direccion'] != 0, 'direccion'] = 1\n",
    "\n",
    "df['direccion'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Titulo y Descripcion:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['titulo'] = df['titulo'].fillna(value = \".\")\n",
    "df['titulo'] = df['titulo'].apply(normalizar_texto)\n",
    "df['descripcion'] = df['descripcion'].fillna(value = \".\")\n",
    "df['descripcion'] = df['descripcion'].apply(normalizar_texto)\n",
    "\n",
    "df['calefaccion'] = df['titulo'].apply(contar_palabras, palabras=[\"calefaccion\", \"calefacción\", \"calefaccionado\", \"aireacondicinado\",\n",
    "                    \"acondicionado\", \"estufa\",\"chimenea\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"calefaccion\", \n",
    "                    \"calefacción\", \"calefaccionado\", \"aireacondicinado\", \"acondicionado\", \"estufa\",\"chimenea\"])\n",
    "\n",
    "df['suite'] = df['titulo'].apply(contar_palabras, palabras=[\"suite\"])+df['descripcion'].apply(contar_palabras, palabras=[\"suite\"])\n",
    "\n",
    "df['avenida'] = df['titulo'].apply(contar_palabras, palabras=[\"avenida\", \"av\", \"avenidas\", \"bulevar\", \n",
    "               \"boulevard\", \"paseo\", \"vía\"])+df['descripcion'].apply(contar_palabras, palabras=[\"avenida\", \"av\", \n",
    "                \"avenidas\", \"bulevar\",\"boulevard\", \"paseo\", \"vía\"])\n",
    "\n",
    "df['gim'] = df['titulo'].apply(contar_palabras, palabras=[\"gimnasio\", \"gimnásio\", \"entrenamiento\", \"gim\", \"gym\", \"fit\",\n",
    "            \"ejercicio\", \"gimnasia\",\"atletismo\", \"cancha\"])+df['descripcion'].apply(contar_palabras, palabras=[\"gimnasio\",\n",
    "            \"gimnásio\", \"entrenamiento\", \"gim\", \"gym\", \"fit\",\"ejercicio\", \"gimnasia\",\"atletismo\", \"cancha\"])\n",
    "\n",
    "df['cochera'] = df['titulo'].apply(contar_palabras, palabras=[\"cochera\", \"cocheras\", \"garage\", \n",
    "                \"garages\", \"garaje\", \"garajes\"])+df['descripcion'].apply(contar_palabras, palabras=[\"cochera\", \n",
    "                \"cocheras\", \"garage\",\"garages\", \"garaje\", \"garajes\"])\n",
    "\n",
    "df['ubicacion'] = df['titulo'].apply(contar_palabras, palabras=[\"ubicacion\", \"ubicación\", \"locacion\", \n",
    "                \"locación\", \"localizacion\", \"localización\",\"ubicado\", \"ubicada\", \"centro\", \"centrico\",\n",
    "                \"centrica\", \"céntrico\", \"céntrica\",\"central\"])+df['descripcion'].apply(contar_palabras, palabras=[\"ubicacion\", \"ubicación\", \"locacion\", \n",
    "                \"locación\", \"localizacion\", \"localización\",\"ubicado\", \"ubicada\", \"centro\", \"centrico\",\n",
    "                \"centrica\", \"céntrico\", \"céntrica\",\"central\"])\n",
    "\n",
    "df['balcon'] = df['titulo'].apply(contar_palabras, palabras=[\"balcon\", \"balcón\", \"terraza\", \"palco\", \"mirador\", \n",
    "            \"balconcillo\",\"azotea\", \"solana\"])+df['descripcion'].apply(contar_palabras, palabras=[\"balcon\", \"balcón\", \n",
    "            \"terraza\", \"palco\", \"mirador\", \"balconcillo\",\"azotea\", \"solana\"])\n",
    "\n",
    "df['camaraseg'] = df['titulo'].apply(contar_palabras, palabras=[\"camara\", \"cámara\", \"cámaras\", \"camaras\",\"seguridad\",\n",
    "                \"guardia\",\"seguro\",\"protegido\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"camara\", \"cámara\",\n",
    "                \"cámaras\", \"camaras\",\"seguridad\",\"guardia\",\"seguro\",\"protegido\"])\n",
    "\n",
    "df['parque'] = df['titulo'].apply(contar_palabras, palabras=[\"parque\", \"plaza\", \"plazoleta\", \"glorieta\",\"jardin\",\n",
    "            \"jardín\",\"patio\"])+df['descripcion'].apply(contar_palabras, palabras=[\"parque\", \"plaza\", \"plazoleta\", \n",
    "            \"glorieta\",\"jardin\",\"jardín\",\"patio\"])\n",
    "\n",
    "df['amoblado'] = df['titulo'].apply(contar_palabras, palabras=[\"muebles\", \"amoblado\", \"mueble\",\"decorado\",\n",
    "                \"listo\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"muebles\", \"amoblado\", \"mueble\",\n",
    "                \"decorado\",\"listo\"])\n",
    "\n",
    "df['bañera'] = df['titulo'].apply(contar_palabras, palabras=[\"bañera\", \"hidromasaje\", \"hidro\", \"tina\",\n",
    "            \"jacuzzi\",\"jacuzi\",\"yacuzi\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"bañera\", \"hidromasaje\", \n",
    "            \"hidro\", \"tina\",\"jacuzzi\",\"jacuzi\",\"yacuzi\"])\n",
    "\n",
    "\n",
    "df['estreno'] = df['titulo'].apply(contar_palabras, palabras=[\"nuevo\", \"nueva\", \"estrenar\",\"estreno\",\n",
    "            \"innovador\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"nuevo\", \"nueva\", \"estrenar\",\"estreno\",\n",
    "            \"innovador\"])\n",
    "\n",
    "df['transporte'] = df['titulo'].apply(contar_palabras, palabras=[\"subte\", \"subterraneo\", \"subterráneo\",\"metro\", \n",
    "                \"estacion\", \"estación\", \"tren\",\"subestacion\", \"subestación\", \"ferrocarril\",\"metrobús\", \"metrobus\", \n",
    "                \"trolebus\",\"trolebús\", \"bus\", \"bús\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"subte\", \n",
    "                \"subterraneo\", \"subterráneo\",\"metro\", \"estacion\", \"estación\", \"tren\",\"subestacion\", \"subestación\", \n",
    "                \"ferrocarril\",\"metrobús\", \"metrobus\", \"trolebus\",\"trolebús\", \"bus\", \"bús\"])\n",
    "\n",
    "df['pileta'] = df['titulo'].apply(contar_palabras, palabras=[\"piscina\", \"pileta\", \"nado\"])+ df['descripcion'].apply(contar_palabras, palabras=[\"piscina\", \"pileta\", \"nado\"])\n",
    "                                       \n",
    "df['lujo'] = df['titulo'].apply(contar_palabras, palabras=[\"lujo\", \"delujo\", \"deluxe\", \"delúxe\", \"lujosa\", \"lujoso\", \n",
    "            \"lujosas\", \"lujosos\",\"exclusivo\",\"vip\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"lujo\", \n",
    "            \"delujo\",\"deluxe\", \"delúxe\", \"lujosa\", \"lujoso\", \"lujosas\", \"lujosos\",\"exclusivo\",\"vip\"])\n",
    "\n",
    "df['humilde'] = df['titulo'].apply(contar_palabras, palabras=[\"humilde\", \"economico\", \"economica\", \n",
    "            \"económico\", \"económica\", \"barata\", \"barato\", \"accesible\", \"baratillo\",\n",
    "            \"baratilla\", \"rebajado\", \"ganga\", \"asequible\", \"módico\", \"módica\",\"credito\",\"crédito\",\"oferta\",\"oferton\",\n",
    "            \"imperdible\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"humilde\", \"economico\", \"economica\", \n",
    "            \"económico\", \"económica\", \"barata\", \"barato\", \"accesible\", \"baratillo\",\n",
    "            \"baratilla\", \"rebajado\", \"ganga\", \"asequible\", \"módico\", \"módica\",\n",
    "            \"credito\",\"crédito\",\"oferta\",\"oferton\",\"imperdible\"]) \n",
    "\n",
    "df['ventana'] = df['titulo'].apply(contar_palabras, palabras=[\"ventana\", \"ventanas\", \n",
    "            \"vista\", \"ventanal\",\"vistas\",\"cristal\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"ventana\",\n",
    "            \"ventanas\", \"vista\", \"ventanal\",\"vistas\",\"cristal\"])\n",
    "\n",
    "df['nuevo'] = df['titulo'].apply(contar_palabras, palabras=[\"reciente\", \"recien\", \"recién\", \"nueva\", \"nuevo\", \"nuevas\", \n",
    "            \"nuevos\", \"estrenar\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"reciente\", \"recien\", \"recién\", \n",
    "            \"nueva\", \"nuevo\", \"nuevas\", \"nuevos\", \"estrenar\"])\n",
    "\n",
    "df['luz'] = df['titulo'].apply(contar_palabras, palabras=[\"luz\", \"luminoso\", \"luminosa\",\"claridad\", \"luminiscencia\", \n",
    "        \"luminosidad\", \"iluminación\",\"iluminacion\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"luz\", \n",
    "        \"luminoso\", \"luminosa\",\"claridad\", \"luminiscencia\",\"luminosidad\", \"iluminación\",\"iluminacion\"])\n",
    "\n",
    "df['bueno'] = df['titulo'].apply(contar_palabras, palabras=[\"bueno\", \"buena\", \"buenas\", \"buenos\",\"excelente\", \n",
    "            \"excelentes\",\"increible\",\"espectacular\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"bueno\", \n",
    "            \"buena\", \"buenas\", \"buenos\",\"excelente\", \"excelentes\",\"increible\",\"espectacular\"])\n",
    "\n",
    "df['contable'] = df['titulo'].apply(contar_palabras, palabras=[\"precio\"]) + df['descripcion'].apply(contar_palabras, palabras=[\"precio\"])\n",
    "\n",
    "#Nuevos Feat\n",
    "\n",
    "df['agente'] = df['descripcion'].apply(contar_palabras, palabras=[\"inmobiliaria\", \"asesoria\", \"asesoría\", \"lider\", \"re/max\", \"remax\"]) \\\n",
    "             + df['titulo'].apply(contar_palabras, palabras=[\"inmobiliaria\", \"asesoria\", \"asesoría\", \"lider\", \"re/max\", \"remax\"])\n",
    "\n",
    "df['garante'] = df['descripcion'].apply(contar_palabras, palabras=[\"garante\", \"garantía\", \"fiador\", \"garantizador\", \"avalista\", \"garantia\",\n",
    "                \"defensor\", \"garantías\", \"garantes\", \"codeudor\"]) + df['titulo'].apply(contar_palabras, palabras=[\"garante\", \n",
    "                \"garantía\", \"fiador\", \"garantizador\", \"avalista\", \"garantia\", \"defensor\", \"garantías\", \"garantes\", \"codeudor\"])\n",
    "\n",
    "df['finanza'] =  df['descripcion'].apply(contar_palabras, palabras=[\"credito\", \"crédito\", \"prestamo\", \"préstamo\", \"cuotas\", \"pagos\", \"hipotecario\"\\\n",
    "                 ,\"amortizable\", \"aptocredito\", \"aptocrédito\", \"apto-credito\", \"apto-crédito\", \"aptocredito\", \"aptocréditp\", \"ahorro\"]) + \\\n",
    "                 df['titulo'].apply(contar_palabras, palabras=[\"credito\", \"crédito\", \"prestamo\", \"préstamo\", \"cuotas\", \"pagos\", \"hipotecario\" \\\n",
    "                 ,\"amortizable\", \"aptocredito\", \"aptocrédito\", \"apto-credito\", \"apto-crédito\", \"aptocredito\", \"aptocrédito\", \"ahorro\"]) \n",
    "\n",
    "df['turismo'] =  df['descripcion'].apply(contar_palabras, palabras=[\"playa\", \"vacaciones\", \"descanso\", \"costa\", \"arena\", \"mar\", \"montaña\", \"monte\",\n",
    "                                                                   \"paisaje\", \"orilla\", \"rambla\", \"turista\", \"turistas\", \"cordillera\", \"sierra\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"playa\", \"vacaciones\", \"descanso\", \"costa\", \"arena\", \"mar\", \"montaña\", \"monte\",\n",
    "                                                                   \"paisaje\", \"orilla\", \"rambla\", \"turista\", \"turistas\", \"cordillera\", \"sierra\"])\n",
    "\n",
    "df['longitud_desc'] =  df['descripcion'].apply(lambda x: len(x.split()))\n",
    "\n",
    "#Nuevas de V4\n",
    "\n",
    "df['longitud_titulo'] = df['titulo'].apply(lambda x: len(x))\n",
    "\n",
    "df['cant_!'] = df['titulo'].apply(lambda x: x.count('!')) \n",
    "\n",
    "df['tranquilidad'] = df['descripcion'].apply(contar_palabras, palabras=[\"armonía\", \"armonia\", \"tranquilo\", \"tranqui\", \"tranquilidad\", \\\n",
    "                                                                        \"paz\", \"calma\", \"calmo\", \"quietud\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"armonía\", \"armonia\", \"tranquilo\", \"tranqui\", \"tranquilidad\", \\\n",
    "                                                                        \"paz\", \"calma\", \"calmo\", \"quietud\"])\n",
    "\n",
    "df['reparacion'] = df['descripcion'].apply(contar_palabras, palabras=[\"reparación\", \"reparacion\", \"reparando\", \"reparar\", \\\n",
    "                            \"construcción\", \"construccion\", \"construyendo\", \"construllendo\", \"mantenimiento\", \"averiado\", \"averiada\",\n",
    "                                                                     \"refaccion\",\"refacción\",\"refacciones\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"reparación\", \"reparacion\", \"reparando\", \"reparar\", \\\n",
    "                            \"construcción\", \"construccion\", \"construyendo\", \"construllendo\", \"mantenimiento\", \"averiado\", \"averiada\"\n",
    "                                                                     \"refaccion\",\"refacción\",\"refacciones\"])\n",
    "\n",
    "df['mascotas'] = df['descripcion'].apply(contar_palabras, palabras=[\"mascotas\", \"mascota\", \"perros\", \"perro\", \"perra\", \\\n",
    "                                                \"gatos\", \"gato\", \"gata\", \"animal\", \"animales\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"mascotas\", \"mascota\", \"perros\", \"perro\", \"perra\", \\\n",
    "                                                \"gatos\", \"gato\", \"gata\", \"animal\", \"animales\"])\n",
    "\n",
    "df['accesibilidad'] = df['descripcion'].apply(contar_palabras, palabras=[\"rampa\", \"discapacitados\", \"discapacitado\", \\\n",
    "                                                \"discapacitada\", \"lisiado\", \"lisiada\", \"silla de ruedas\", \"lastimado\", \"heridos\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"rampa\", \"discapacitados\", \"discapacitado\", \\\n",
    "                                                \"discapacitada\", \"lisiado\", \"lisiada\", \"silla de ruedas\", \"lastimado\", \"heridos\"])\n",
    "\n",
    "df['normas'] =  df['descripcion'].apply(contar_palabras, palabras=[\"norma\", \"normas\", \"regla\", \"reglas\", \\\n",
    "                                                \"prohibido\", \"prohibida\", \"denegado\", \"denegada\", \"imposible\", \\\n",
    "                                                \"ilegal\", \"legal\", \"multa\", \"infraccion\", \"infracción\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"norma\", \"normas\", \"regla\", \"reglas\", \\\n",
    "                                                \"prohibido\", \"prohibida\", \"denegado\", \"denegada\", \"imposible\", \\\n",
    "                                                \"ilegal\", \"legal\", \"multa\", \"infraccion\", \"infracción\"])\n",
    "\n",
    "df['beneficios'] = df['descripcion'].apply(contar_palabras, palabras=[\"gratis\", \"free\", \"incluido\", \"incluye\", \\\n",
    "                                                \"agregado\", \"gratuito\", \"gratuitamente\", \\\n",
    "                                                \"regalo\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"gratis\", \"free\", \"incluido\", \"incluye\", \\\n",
    "                                                \"agregado\", \"gratuito\", \"gratuitamente\", \\\n",
    "                                                \"regalo\"])\n",
    "\n",
    "df['conexion'] = df['descripcion'].apply(contar_palabras, palabras=[\"wifi\", \"wi-fi\", \"internet\", \"conexión\", \"conexion\", \\\n",
    "                                                                   \"señal\"])\\\n",
    "                 + df['titulo'].apply(contar_palabras, palabras=[\"wifi\", \"wi-fi\", \"internet\", \"conexión\", \"conexion\", \\\n",
    "                                                                   \"señal\"])\n",
    "\n",
    "df['servicios_desc'] = df['descripcion'].apply(contar_palabras, palabras = [\"servicio\", \"servicios\"])\n",
    "\n",
    "df['metros_desc'] = df['descripcion'].apply(contar_palabras, palabras = [\"metros\",\"m2\"])\n",
    "\n",
    "df['acabados'] = df['descripcion'].apply(contar_palabras, palabras = [\"acabados\",\"acabado\",\"terminacion\",\"terminación\"])\n",
    "\n",
    "df['plusvalia'] = df['descripcion'].apply(contar_palabras, palabras = [\"plusvalia\", \"plusvalía\"])\n",
    "\n",
    "#Nuevos de V5\n",
    "\n",
    "df['cocina'] = df['descripcion'].apply(contar_palabras, palabras=['cocina', 'cocinas'])\n",
    "\n",
    "df['alberca'] = df['descripcion'].apply(contar_palabras, palabras=['alberca'])\n",
    "\n",
    "df['negacion'] = df['descripcion'].apply(contar_palabras, palabras = [\"no\"])\n",
    "\n",
    "df['variospisos'] = df['descripcion'].apply(contar_palabras, palabras = [\"escalera\",\"escaleras\",\n",
    "                    \"ascensor\", \"elevador\", \"escalinata\", \"gradas\", \"escalerilla\"])\n",
    "\n",
    "df['vestidor'] = df['descripcion'].apply(contar_palabras, palabras = [\"vestidor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Metros:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['metrosdescubiertos'] = abs(df['metrostotales']-df['metroscubiertos'])\n",
    "df['relacionmetros'] = df['metrostotales']/df['metroscubiertos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_split (df):\n",
    "\n",
    "    X = df.drop(['precio'], axis = 1)\n",
    "    Y = df['precio']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.35)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def minmax(serie):\n",
    "    \n",
    "    return (serie-serie.min())/(serie.max()-serie.min())\n",
    "\n",
    "def mean_target_encoding(train, nombrefeat, nombretarget):\n",
    "    \n",
    "    temp = train.groupby(nombrefeat)[nombretarget].transform(np.mean)\n",
    "    train[nombrefeat + \"_mean\"]=(temp-temp.min())/(temp.max()-temp.min())\n",
    "    \n",
    "    return train\n",
    "\n",
    "def mean_target_decoding(x_test, nombrefeat, x_train):\n",
    "    \n",
    "    nombrefeatmean = nombrefeat + \"_mean\"\n",
    "    \n",
    "    temp = x_train.loc[:,[nombrefeat,nombrefeatmean]]\n",
    "    temp = temp.set_index(nombrefeat)\n",
    "    temp = temp.drop_duplicates()\n",
    "    temp = temp.T.squeeze()\n",
    "    values = x_test[nombrefeat].map(temp)\n",
    "    x_test[nombrefeatmean] = values \n",
    "\n",
    "    return x_test\n",
    "\n",
    "def armar_set_2(train):\n",
    "    \n",
    "    #Puedo resolver de forma general las que son iguales para train y test\n",
    "    #es decir, aquellas que no filtran informacion a los de validacion\n",
    "    \n",
    "    train = train.drop(['descripcion', 'titulo', 'direccion', 'id'], axis = 1)\n",
    "    \n",
    "    #Adiciono servicios\n",
    "    train['servicios'] = train['piscina'] + train['gimnasio'] + train['usosmultiples']\n",
    "    \n",
    "    train['fecha'] = pd.to_datetime(train['fecha']).dt.year\n",
    "    \n",
    "    #\"Normalizo\" la antiguedad\n",
    "    #train['antiguedad'] = minmax(train['antiguedad'])\n",
    "    \n",
    "    #Elimino los residuos\n",
    "    #train = train.drop(['piscina', 'gimnasio','usosmultiples','escuelascercanas', \n",
    "                       # 'centroscomercialescercanos'], axis = 1)\n",
    "    \n",
    "    #Hago el split                    \n",
    "    x_train, x_test, y_train, y_test = mi_split(train)\n",
    "        \n",
    "    x_train[\"precio\"] = y_train\n",
    "    x_test[\"precio\"] = y_test\n",
    "    \n",
    "    \n",
    "    df1 = x_train.dropna(subset=['metrostotales'])\n",
    "    df1['preciometro']=df1['precio']/df1['metrostotales']\n",
    "\n",
    "    PMM = df1.groupby('idzona').agg({'preciometro': 'mean'})\n",
    "    PMM = PMM.reset_index()\n",
    "    PMM = PMM.rename(columns={\"preciometro\": \"precioMetroMean\"})\n",
    "\n",
    "    x_train = x_train.merge(PMM, on='idzona',how='left')\n",
    "    x_test = x_test.merge(PMM, on='idzona',how='left')\n",
    "\n",
    "\n",
    "    df1 = x_train.dropna(subset=['metrostotales'])\n",
    "    df1['preciometroCub']=df1['precio']/df1['metroscubiertos']\n",
    "\n",
    "    PMM = df1.groupby('idzona').agg({'preciometroCub': 'mean'})\n",
    "    PMM = PMM.reset_index()\n",
    "    PMM = PMM.rename(columns={\"preciometroCub\": \"precioMetroCubMean\"})\n",
    "\n",
    "    x_train = x_train.merge(PMM, on='idzona',how='left')\n",
    "    x_test = x_test.merge(PMM, on='idzona',how='left')\n",
    "    \n",
    "    \n",
    "    \n",
    "    df1 = x_train.dropna(subset=['metrostotales'])\n",
    "    df1['preciometroDes']=df1['precio']/df1['metrosdescubiertos']\n",
    "\n",
    "    PMM = df1.groupby('idzona').agg({'preciometroDes': 'mean'})\n",
    "    PMM = PMM.reset_index()\n",
    "    PMM = PMM.rename(columns={\"preciometroDes\": \"precioMetroDesMean\"})\n",
    "\n",
    "    x_train = x_train.merge(PMM, on='idzona',how='left')\n",
    "    x_test = x_test.merge(PMM, on='idzona',how='left')\n",
    "    \n",
    "    \n",
    "    #x = x_test.fillna(0)\n",
    "    #x_test['primera_pred'] = (x['precioMetroCubMean']*x['metroscubiertos']+x['precioMetroMean']*x['metrostotales'])/2\n",
    "    \n",
    "    #x = x_train.fillna(0)\n",
    "    #x_train['primera_pred'] = (x['precioMetroCubMean']*x['metroscubiertos']+x['precioMetroMean']*x['metrostotales'])/2\n",
    "\n",
    "    \n",
    "    df1 = x_train.dropna(subset=['metrostotales'])\n",
    "    df1['preciometro']=df1['precio']/df1['metrostotales']\n",
    "    \n",
    "    PMM = df1.groupby('ciudad').agg({'preciometro': 'mean'})\n",
    "    PMM = PMM.reset_index()\n",
    "    PMM = PMM.rename(columns={\"preciometro\": \"precioMetroMeanC\"})\n",
    "\n",
    "    x_train = x_train.merge(PMM, on='ciudad',how='left')\n",
    "    x_test = x_test.merge(PMM, on='ciudad',how='left')\n",
    "    \n",
    "    \n",
    "    df1 = x_train.dropna(subset=['metroscubiertos'])\n",
    "    df1['preciometroCub']=df1['precio']/df1['metroscubiertos']\n",
    "\n",
    "    PMM = df1.groupby('ciudad').agg({'preciometroCub': 'mean'})\n",
    "    PMM = PMM.reset_index()\n",
    "    PMM = PMM.rename(columns={\"preciometroCub\": \"precioMetroCubMeanC\"})\n",
    "\n",
    "    x_train = x_train.merge(PMM, on='ciudad',how='left')\n",
    "    x_test = x_test.merge(PMM, on='ciudad',how='left')\n",
    "    \n",
    "    \n",
    "    df1 = x_train.dropna(subset=['metrosdescubiertos'])\n",
    "    df1['preciometroDes']=df1['precio']/df1['metrosdescubiertos']\n",
    "\n",
    "    PMM = df1.groupby('ciudad').agg({'preciometroDes': 'mean'})\n",
    "    PMM = PMM.reset_index()\n",
    "    PMM = PMM.rename(columns={\"preciometroDes\": \"precioMetroDesMeanC\"})\n",
    "\n",
    "    x_train = x_train.merge(PMM, on='ciudad',how='left')\n",
    "    x_test = x_test.merge(PMM, on='ciudad',how='left')\n",
    "    \n",
    "    \n",
    "    #Calculo los mean target\n",
    "    x_train_mean = mean_target_encoding(x_train, \"provincia\", \"precio\") \n",
    "    x_train_mean['precio'] = y_train\n",
    "    x_train_mean = mean_target_encoding(x_train_mean, \"tipodepropiedad\", \"precio\")\n",
    "    x_train_mean = mean_target_encoding(x_train_mean, \"ciudad\", \"precio\")\n",
    "    x_train = mean_target_encoding(x_train_mean, \"idzona\", \"precio\")\n",
    "\n",
    "    \n",
    "    #Se los asigno a los test (NO LOS CALCULO CON ELLOS!!!!!!)\n",
    "    x_test = mean_target_decoding(x_test, \"provincia\", x_train_mean)\n",
    "    x_test = mean_target_decoding(x_test, \"tipodepropiedad\", x_train_mean)\n",
    "    x_test = mean_target_decoding(x_test, \"ciudad\", x_train_mean)\n",
    "    x_test = mean_target_decoding(x_test, \"idzona\", x_train_mean)\n",
    "    \n",
    "    backup = x_train_mean\n",
    "    \n",
    "    x_train = x_train_mean.drop([\"precio\",\"provincia\",\"tipodepropiedad\",\"ciudad\"], axis=1)\n",
    "    x_test.drop([\"precio\",\"provincia\",\"tipodepropiedad\", \"ciudad\"], axis=1, inplace = True)\n",
    "\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giaco/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/giaco/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/giaco/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/giaco/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/giaco/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/giaco/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, backup = armar_set_2(df)\n",
    "\n",
    "y = y_train.values\n",
    "\n",
    "x = x_train.values\n",
    "\n",
    "y_test = y_test.values\n",
    "\n",
    "x_test = x_test.values\n",
    "\n",
    "# divido el df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo LightGBM data containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_features = [c for c, col in enumerate(train.columns) if 'cat' in col]\n",
    "train_data = lightgbm.Dataset(x, label=y, categorical_feature=categorical_features)\n",
    "test_data = lightgbm.Dataset(x_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giaco/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/giaco/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For early stopping, at least one dataset and eval metric is required for evaluation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-abc74777de54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                        \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                        \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                        early_stopping_rounds=100)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    262\u001b[0m                                         \u001b[0mbegin_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_iteration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                                         \u001b[0mend_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                                         evaluation_result_list=evaluation_result_list))\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_callback\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcmp_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             raise ValueError('For early stopping, '\n\u001b[0m\u001b[1;32m    196\u001b[0m                              'at least one dataset and eval metric is required for evaluation')\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For early stopping, at least one dataset and eval metric is required for evaluation"
     ]
    }
   ],
   "source": [
    "#Lo entreno\n",
    "\n",
    "params = {\n",
    "    'colsample_bytree': 0.9731668400523877,\n",
    "    'num_leaves': 41,\n",
    "    'subsample': 0.5575732396028996,\n",
    "    'n_estimators' : 3000,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l1', 'l2'},\n",
    "    'learning_rate': 0.3,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 1,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 171,\n",
    "    'min_child_weight': 1e-05,\n",
    "    'reg_alpha': 10,\n",
    "    'reg_lambda': 100,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "model = lightgbm.train(params,\n",
    "                       train_data,\n",
    "                       valid_sets=test_data,\n",
    "                       num_boost_round=5000,\n",
    "                       early_stopping_rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531924.0446470394\n"
     ]
    }
   ],
   "source": [
    "#Estimo MAE\n",
    "print(mean_absolute_error(y_test, prediccion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise-deprecating',\n",
       "             estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "                                     colsample_bytree=1.0,\n",
       "                                     importance_type='split', learning_rate=0.1,\n",
       "                                     max_bin=512, max_depth=-1,\n",
       "                                     min_child_samples=5, min_child_weight=1,\n",
       "                                     min_split_gain=0.5, n_estimators=100,\n",
       "                                     n_jobs=None, num_leaves=31,\n",
       "                                     objective='binary', random_state=None,\n",
       "                                     reg_alpha=0...\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'boosting_type': ['gbdt'],\n",
       "                         'colsample_bytree': [0.65, 0.66],\n",
       "                         'learning_rate': [0.005], 'n_estimators': [1, 2],\n",
       "                         'num_leaves': [6, 8, 12, 16],\n",
       "                         'objective': ['regression'], 'random_state': [501],\n",
       "                         'reg_alpha': [1, 1.2], 'reg_lambda': [1, 1.2, 1.4],\n",
       "                         'subsample': [0.7, 0.75]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'regression',\n",
    "          'nthread': None, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error'}\n",
    "\n",
    "# creo los parametros para grid  \n",
    "gridParams = {\n",
    "    'learning_rate': [0.005],\n",
    "    'n_estimators': [1,2],\n",
    "    'num_leaves': [6,8,12,16],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['regression'],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "# pongo los parametros \n",
    "mdl = lgb.LGBMRegressor(boosting_type= 'gbdt',\n",
    "          objective = 'regression',\n",
    "          n_jobs = None, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "#default:\n",
    "mdl.get_params().keys()\n",
    "\n",
    "# Armo grid\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=0,\n",
    "                    cv=4,\n",
    "                    n_jobs=None)\n",
    "# Enteno\n",
    "grid.fit(x, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.65,\n",
       " 'learning_rate': 0.005,\n",
       " 'n_estimators': 2,\n",
       " 'num_leaves': 16,\n",
       " 'objective': 'regression',\n",
       " 'random_state': 501,\n",
       " 'reg_alpha': 1,\n",
       " 'reg_lambda': 1,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600921.908940707\n"
     ]
    }
   ],
   "source": [
    "#Estimo MAE\n",
    "print(mean_absolute_error(y_test, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-94-41a49510c087>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-94-41a49510c087>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    536821.0451614114 100\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "561929.2799609166 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f67e04dec90>, 'min_child_samples': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f67d9161750>, 'min_child_weight': [1e-05, 0.001, 0.01, 0.1, 1, 10.0, 100.0, 1000.0, 10000.0], 'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f67d91616d0>, 'colsample_bytree': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f67da3ffa90>, 'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10, 50, 100], 'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50, 100]}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "print (param_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numero de ptos a probar \n",
    "n_HP_points_to_test = 3\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='mae', n_jobs=None, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 10.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score with params: {'colsample_bytree': 0.9731668400523877, 'min_child_samples': 171, 'min_child_weight': 1e-05, 'num_leaves': 41, 'reg_alpha': 10, 'reg_lambda': 100, 'subsample': 0.5575732396028996} \n"
     ]
    }
   ],
   "source": [
    "gs.fit(x, y_train)\n",
    "print('Best score with params: {} '.format( gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediccion = gs.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509839.05516658636\n"
     ]
    }
   ],
   "source": [
    "#Estimo MAE\n",
    "print(mean_absolute_error(y_test, prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
